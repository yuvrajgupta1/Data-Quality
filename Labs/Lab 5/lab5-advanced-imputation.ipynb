{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Advanced Missing Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Regression Models\n",
    "\n",
    "In the previous lecture, we discussed advanced regression models including neural networks and k-nearest neighbours (kNN). But in the previous lab, we only looked at using linear regression in sklearn. Now we'll try the more advanced models. Fortunately, sklearn makes it very easy to swap in different machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the same college dataset we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('College-MISSING.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict graduation rate (Grad.Rate) based on the other variables. So Grad.Rate will be our outcome (y) and the other variables will be our features (X). \n",
    "\n",
    "But first we will remove missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Missing Data\n",
    "\n",
    "We will remove any rows with missing (NA) data, in order to fit our advanced regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df.dropna(axis=0, how='any')\n",
    "print(df_complete.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will divide that into features (X) and outcomes (y), as before. We also add a line of code that scales the variables (preprocessing.scale(X)), as neural networks usually perform better when all the features are on roughly the same scale. If you remove this, you will likely see performance go down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_complete.drop(['College', 'Private', 'Grad.Rate'], axis=1)\n",
    "X = pd.DataFrame(preprocessing.scale(X), columns=X.columns)\n",
    "print(\"Here are the features (X):\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\n\\nHere is the outcome variable (y):\")\n",
    "y = df_complete['Grad.Rate']\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_complete.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Regression Models in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lecture, we described a simple Perceptron as being like a single neuron, and a neural network being an extension of the Perceptron where we have multiple neurons arranged in layers, including hidden layers between the inputs and outputs. So multi-layer perceptron (or MLP) is just another term for neural network.\n",
    "\n",
    "We will fit a neural network regression model to predict Grad.Rate, just like in the previous lab.\n",
    "\n",
    "\n",
    "We create an instance of the MLPRegressor class in sklearn. We need to specify how many hidden layers it should have and how many neurons in each of those layers. In this case, it's three hidden layers, with sizes of 100, 100, and 50, respectively. The other arguments specify which training algorithm it should use for learning the neural network weights, and the maximum number of iterations it should try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1 = MLPRegressor(hidden_layer_sizes=(100,100,50), solver='lbfgs', max_iter=500, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit the model to the training data. Thankfully, sklearn provides a consistent API for the various machine learning models. So training and prediction is just the same as it was before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a kNN regression model. We can specify the value of _k_ or use the default of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = KNeighborsRegressor(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit that model to the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imputation\n",
    "\n",
    "Then we want to get predictions on the full dataset, including the rows that had missing (NA) data, so that we can impute the missing values using our trained regression model. \n",
    "\n",
    "So we need to get the features (X) for the entire dataset, not just the complete cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df.drop(['College', 'Private', 'Grad.Rate'], axis=1)\n",
    "X_all = pd.DataFrame(preprocessing.scale(X_all), columns=X_all.columns)\n",
    "\n",
    "preds1 = lm1.predict(X_all)\n",
    "\n",
    "preds2 = lm2.predict(X_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could get predictions for just the records where Grad.Rate was missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df['Grad.Rate'].isnull()\n",
    "\n",
    "preds_missing1 = lm1.predict(X_all.loc[missing, :])\n",
    "\n",
    "preds_missing2 = lm2.predict(X_all.loc[missing, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "Let's do a similar step as you did in the first part of your lab assignment last week. We will look at the gold-standard Grad.Rate values in _College.csv_ and see how our predictions compare. But this time we will only look at the subset of the data that was missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.read_csv('College.csv')\n",
    "gs_grad = gs_df.loc[missing, 'Grad.Rate']\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(preds_missing1, gs_grad)\n",
    "plt.xlabel(\"Predicted Grad Rate (NN)\")\n",
    "plt.ylabel(\"Actual Grad Rate\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(preds_missing2, gs_grad)\n",
    "plt.xlabel(\"Predicted Grad Rate (kNN)\")\n",
    "plt.ylabel(\"Actual Grad Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be difficult to assess which approach is doing better just by looking at the scatter plots. \n",
    "\n",
    "Let's calculate the mean-squared error (MSE) for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = sum((gs_grad - preds_missing1)**2) / len(preds_missing1)\n",
    "print(\"MSE with MLP regressor: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = sum((gs_grad - preds_missing2)**2) / len(preds_missing2)\n",
    "print(\"MSE with kNN regressor: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results may differ slightly, but you should see that kNN performs better on this task. However, you might get significant improvement by trying different numbers of hidden layers and neurons. Neural networks also tend to perform better on larger datasets. If you remove the lines of code that do feature scaling, you should see neural networks perform much worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lab Assignment__: You are provided with a dataset _winequality-MISSING.csv_ that has some data about different wines, and how each wine was rated. Some of the wine ratings are missing. Try fitting advanced regression models using neural networks and kNN to predict the missing rating values. \n",
    "\n",
    "Then load in the gold-standard data in winequality.csv and compare your predicted values for the missing ratings with the actual ratings. Show scatterplots of the predicted vs. actual ratings for the missing data, for both models. Also show the MSE for each predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
